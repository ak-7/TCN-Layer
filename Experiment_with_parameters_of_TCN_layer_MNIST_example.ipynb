{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment with parameters of TCN layer - MNIST example.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TcFQu3F0y-fy",
        "ByuyggHey-gI",
        "xdTffJqQy-gU",
        "WhzHzZtMy-gk"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak-7/TCN-Layer/blob/main/Experiment_with_parameters_of_TCN_layer_MNIST_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b10XJFD-y-fS"
      },
      "source": [
        "# Solving Sequential MNIST with Temporal Convolutional Networks(TCNs)\n",
        "\n",
        "- Sequential MNIST: Based on the work of [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples/) and [Sungjoon](https://github.com/sjchoi86/tensorflow-101/blob/master/notebooks/rnn_mnist_simple.ipynb)\n",
        "- Temporal Convolutional Networks: [Bai, S., Kolter, J. Z., & Koltun, V. (2018). An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.](http://arxiv.org/abs/1803.01271)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqicde8-y-fW"
      },
      "source": [
        "### MNIST Dataset Overview\n",
        "\n",
        "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
        "\n",
        "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
        "\n",
        "To classify images using a recurrent neural network, we consider every image row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then handle 28 sequences of 28 timesteps for every sample.\n",
        "\n",
        "More info: http://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "### Temporal Convolutional Networks Overview\n",
        "\n",
        "![TCNs](https://cdn-images-1.medium.com/max/1000/1*1cK-UEWHGaZLM-4ITCeqdQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0q2NlroCUA7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "566a8867-8a2a-472f-820e-f093acca774e"
      },
      "source": [
        "32 x 128 x 256\n",
        "\n",
        "\n",
        "\n",
        "input : 32 x 128 x 256\n",
        "kernel, stride = 16, 16\n",
        "conv1: 32 X 8 X 256\n",
        "kernel_stride = 2, 1\n",
        "conv2: 32 X 7 X 256\n",
        "\n",
        "\n",
        "S1 S2 ..................... S128\n",
        "\n",
        "\n",
        "C1 C1 C1 C1 ... C2 C2..             C7 C7 C7..\n",
        "\n",
        "k=16,s=16\n",
        "S1 S2 S3 ..... S128\n",
        "(conv1) S0 - S16 | S16 - S32 | S32 - S48 | S48 - S47\n",
        "(conv2) S0 S32 S16 - 48\n",
        "\n",
        "\n",
        "S1 S2 S3 ..... S128\n",
        "(conv1) S0 - S7 | S8 - S15 | S16 - S31 | S32 - S47\n",
        "(conv2) S0 S15 S16 - 48\n",
        "\n",
        "\n",
        "x: 32 x 8 x 256 (repeat 16 times) 32 x 128 x 256\n",
        "\n",
        "\n",
        "input : 32 x 86 x 256\n",
        "repeat 85th frame input: 32 x 96 x 256\n",
        "\n",
        "x: 32 x 6 x 256 (repeat 16 times) 32 x 96 x 256\n",
        "\n",
        "# Remove last 10 frames from x + original input\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-a6191f8ee9df>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    32 x 128 x 256\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeeOZueJy-fW"
      },
      "source": [
        "## System Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejej7UiFNihR"
      },
      "source": [
        "## TCN experiments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QHUoO6UTLtJ",
        "outputId": "17ac111e-d1fd-4dcd-e2f9-cef404466235"
      },
      "source": [
        " !pip install tensorflow==1.14"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.14 in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (2.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (56.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zj3MnAMy-fq"
      },
      "source": [
        "from pathlib import Path\n",
        "import random \n",
        "from datetime import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Import MNIST data\n",
        "# from tensorflow.examples.tutorials.mnist import input_data\n",
        "# mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79OeEG8-YzXE"
      },
      "source": [
        "kernel_size_1=4\n",
        "strides_1=4\n",
        "kernel_size_2=8\n",
        "strides_2=4\n",
        "conv_maps=256"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8fnqAedyR4P"
      },
      "source": [
        "kernel_size_1=20\n",
        "strides_1=4\n",
        "kernel_size_2=4\n",
        "strides_2=3\n",
        "conv_maps=256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqsTDeH0z9dV"
      },
      "source": [
        "kernel_size_1=20\n",
        "strides_1=4\n",
        "kernel_size_2=2\n",
        "strides_2=3\n",
        "conv_maps=256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4dmcbHIyJ4H"
      },
      "source": [
        "kernel_size_1=84\n",
        "strides_1=4\n",
        "kernel_size_2=8\n",
        "strides_2=4\n",
        "conv_maps=256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhF3O6p59yfP"
      },
      "source": [
        "class CausalConv1D(tf.layers.Conv1D):\n",
        "    def __init__(self, filters,\n",
        "               kernel_size,\n",
        "               strides=1,\n",
        "               dilation_rate=1,\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               kernel_initializer=None,\n",
        "               bias_initializer=tf.zeros_initializer(),\n",
        "               kernel_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               trainable=True,\n",
        "               name=None,\n",
        "               **kwargs):\n",
        "        super(CausalConv1D, self).__init__(\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=strides,\n",
        "            padding='valid',\n",
        "            data_format='channels_last',\n",
        "            dilation_rate=dilation_rate,\n",
        "            activation=activation,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            kernel_constraint=kernel_constraint,\n",
        "            bias_constraint=bias_constraint,\n",
        "            trainable=trainable,\n",
        "            name=name, **kwargs\n",
        "        )\n",
        "       \n",
        "    def call(self, inputs, pad=False):\n",
        "        return super(CausalConv1D, self).call(inputs)\n",
        "\n",
        "\n",
        "\n",
        "class TemporalBlock(tf.layers.Layer):\n",
        "  def __init__(self, n_outputs, kernel_size, strides, dilation_rate, dropout=0.2,\n",
        "               trainable=True, name=None, dtype=None,\n",
        "               activity_regularizer=None, **kwargs):\n",
        "    super(TemporalBlock, self).__init__(\n",
        "      trainable=trainable, dtype=dtype,\n",
        "      activity_regularizer=activity_regularizer,\n",
        "      name=name, **kwargs\n",
        "    )\n",
        "    self.dropout = dropout\n",
        "    self.n_outputs = n_outputs\n",
        "\n",
        "    # Kernel size for first layer\n",
        "    self.kernel_size_1 = kernel_size_1\n",
        "    self.strides_1 = strides_1\n",
        "\n",
        "    ## shift length = stride 1 * stride 2\n",
        "    ## block size = kernel_size1 * kernel_size_2\n",
        "\n",
        "    # Kernel size for second layer\n",
        "    self.kernel_size_2 = kernel_size_2\n",
        "    self.strides_2 = strides_2\n",
        "\n",
        "    self.conv1 = CausalConv1D(\n",
        "      n_outputs, self.kernel_size_1, strides=self.strides_1,\n",
        "      dilation_rate=dilation_rate, activation=tf.nn.relu,\n",
        "      name=\"conv1\")\n",
        "    self.conv2 = CausalConv1D(\n",
        "      n_outputs, self.kernel_size_2, strides=self.strides_2,\n",
        "      dilation_rate=dilation_rate, activation=tf.nn.relu,\n",
        "      name=\"conv2\")\n",
        "    self.down_sample = None\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    channel_dim = 2\n",
        "    self.dropout1 = tf.layers.Dropout(self.dropout, [tf.constant(1), tf.constant(1), tf.constant(self.n_outputs)])\n",
        "    self.dropout2 = tf.layers.Dropout(self.dropout, [tf.constant(1), tf.constant(1), tf.constant(self.n_outputs)])\n",
        "    if input_shape[channel_dim] != self.n_outputs:\n",
        "      # self.down_sample = tf.layers.Conv1D(\n",
        "      #     self.n_outputs, kernel_size=1,\n",
        "      #     activation=None, data_format=\"channels_last\", padding=\"valid\")\n",
        "      self.down_sample = tf.layers.Dense(self.n_outputs, activation=None)\n",
        "    self.built = True\n",
        "\n",
        "  def repeat(self, x, num_repetitions):\n",
        "    # Repeat x element num_repititions times\n",
        "\n",
        "    N = num_repetitions\n",
        "    K = tf.shape(x)[1]\n",
        "    order = tf.range(0, N * K, K)\n",
        "    K_array = tf.range(0, K)\n",
        "\n",
        "    x_ = tf.expand_dims(order, 0)\n",
        "    y_ = tf.expand_dims(K_array, 1)\n",
        "    z = tf.reshape(tf.add(x_, y_), [-1, N])\n",
        "    indices = tf.reshape(z, [-1])\n",
        "    x_rep = tf.gather(tf.tile(x, [1, N, 1]), indices, axis=1)\n",
        "    return x_rep\n",
        "\n",
        "  def call(self, inputs, training=True):\n",
        "    input_length = tf.shape(inputs)[1]\n",
        "    #print('\\nStep1: Input size: ', input_length)\n",
        "    # Transform the input block to multiple of shift len. That involves repeating the last element x number of times\n",
        "    shift_len = self.strides_1 * self.strides_2\n",
        "    # if input_length % shift_len is not None: len_repeat = 0:\n",
        "    len_repeat = shift_len - input_length % shift_len\n",
        "    i_ = self.repeat(inputs[:, input_length - 1:, :], len_repeat)\n",
        "    m_inputs = tf.concat([inputs, i_], 1)\n",
        "    print('Modified inputs: ', m_inputs.shape, 'original: ', inputs.shape)\n",
        "\n",
        "\n",
        "    # Append k1 - s1 frames at beginning for overlapping convolutions came\n",
        "    if self.kernel_size_1 > self.strides_1:\n",
        "      len_repeat = self.kernel_size_1 - self.strides_1\n",
        "      print(\"Add in the beginning: \", len_repeat)\n",
        "      i_ = self.repeat(inputs[:, :1, :], len_repeat)\n",
        "      m_inputs = tf.concat([i_, m_inputs], 1)\n",
        "      print('Step 1.5 Modified inputs: ', m_inputs.shape)\n",
        "    \n",
        "\n",
        "    # then cut what you added in beginning\n",
        "    #print('\\n\\nStep2: Perform dilated convolutions.....')\n",
        "    # Perform dilated convolutions in residual block\n",
        "    x = self.conv1(m_inputs)\n",
        "    x = self.dropout1(x, training=training)\n",
        "    print('X1 shape: ', x.shape) \n",
        "    x = self.conv2(x)\n",
        "    LayerNorm = LayerNormalization()\n",
        "    y1 = LayerNorm(x)\n",
        "    \n",
        "    \n",
        "    y2 = tf.contrib.layers.layer_norm(x, begin_norm_axis=1, center=False, scale=False, trainable=False)\n",
        "    return y1, y2\n",
        "    x = self.dropout2(x, training=training)\n",
        "    print('X2 shape: ', x.shape) \n",
        "\n",
        "    # print('\\n\\nStep3: Transform to input size and perform addition.....')\n",
        "    # Transform output of dilated convolutions to input size. This involves repeating first block x times and the next blocks shift_len times. It also involves removing last x elements\n",
        "    next_blocks_repeat = self.repeat(x, shift_len)\n",
        "    num_repeat = tf.shape(m_inputs)[1] - tf.shape(next_blocks_repeat)[1]\n",
        "    print(m_inputs.shape, next_blocks_repeat.shape)\n",
        "    first_block_repeat = self.repeat(x[:, :1, :],num_repeat)\n",
        "\n",
        "    x = tf.concat([first_block_repeat, next_blocks_repeat], 1)\n",
        "    x = x[:, :input_length, :]\n",
        "    #print('Final X shape after repeating', x.shape)\n",
        "    if self.down_sample is not None:\n",
        "      inputs = self.down_sample(inputs)\n",
        "    return tf.nn.relu(x + inputs)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_IqsnugyWk1"
      },
      "source": [
        "class LayerNormalization(tf.layers.Layer):\n",
        "    \"\"\"Applies layer normalization.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size=256):\n",
        "        super(LayerNormalization, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def build(self, _):\n",
        "        self.scale = tf.get_variable(\"layer_norm_scale\", [self.hidden_size],\n",
        "                                     initializer=tf.ones_initializer())\n",
        "        self.bias = tf.get_variable(\"layer_norm_bias\", [self.hidden_size],\n",
        "                                    initializer=tf.zeros_initializer())\n",
        "        self.built = True\n",
        "    def cum_mean(self, arr):\n",
        "        dim = 1\n",
        "        cum_sum = tf.math.cumsum(arr, axis=dim)   \n",
        "        length_tensor = tf.range(1, tf.shape(cum_sum)[dim] + 1)\n",
        "        length_tensor = tf.cast(length_tensor, tf.float32)\n",
        "        cum_sum = cum_sum / length_tensor\n",
        "        return cum_sum\n",
        "      \n",
        "    def calculate_expanding_mean(self, x):\n",
        "        mean_feature_maps = tf.reduce_mean(x, axis=[-1])\n",
        "        print('Step 1 mean: ', mean_feature_maps.shape)\n",
        "        mean = self.cum_mean(mean_feature_maps)\n",
        "        # print('Step 2 mean: ', mean.shape)\n",
        "        mean = tf.expand_dims(mean, axis=2)\n",
        "        mean = tf.tile(mean, [1, 1, tf.shape(x)[2]])\n",
        "        return mean\n",
        "    \n",
        "    def call(self, x, epsilon=1e-6):\n",
        "        mean = self.calculate_expanding_mean(x)\n",
        "        print('Step 1 mean: ', mean.shape)\n",
        "\n",
        "        epsilon = 0\n",
        "        norm_x_intermediate = tf.square(x - mean)\n",
        "        # variance = self.calculate_expanding_mean(norm_x_intermediate)\n",
        "        variance = tf.reduce_mean(tf.square(x - mean), axis=[-1], keepdims=True)\n",
        "        \n",
        "        print('Step 2 variance: ', variance.shape)\n",
        "        norm_x = (x - mean) * tf.rsqrt(variance + epsilon)\n",
        "        return norm_x\n",
        "        return norm_x * self.scale + self.bias"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbD5fwIE1S_O",
        "outputId": "d7f4f537-750b-4a40-82a5-e9f6fed087eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "tf.compat.v1.random.set_random_seed(1234)\n",
        "with tf.Graph().as_default() as g:\n",
        "    x = tf.random_normal((1, 45, 256)) # (batch_size, length, channel)\n",
        "    # n_outputs, kernel_size, strides, dilation_rate\n",
        "    tblock = TemporalBlock(n_outputs=256, kernel_size=2, strides=1, dilation_rate=1)\n",
        "    output = tblock(x, training=tf.constant(True))\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "    sess.run(init)\n",
        "    res = sess.run(output)\n",
        "    # print(res.shape)\n",
        "    o1, o2 = res\n",
        "    print('Vineet:', o1.shape) \n",
        "    print('TF layer norm:', o2.shape)\n",
        "    print(\"\\n\\n\")\n",
        "    print(type(o1[0][0][0]), type(o2[0][0][1]))\n",
        "    print(o1[0, -1, 240:])\n",
        "    print(\"\\n\\n\")\n",
        "    print(o2[0, -1, 240:])\n",
        "    \n",
        "    print(\"\\n\\n\")\n",
        "    print(o1[0, -2, 240:])\n",
        "    print(\"\\n\\n\")\n",
        "    print(o2[0, -2, 240:])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method TemporalBlock.call of <__main__.TemporalBlock object at 0x7f45092dd350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TemporalBlock.call of <__main__.TemporalBlock object at 0x7f45092dd350>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TemporalBlock.call of <__main__.TemporalBlock object at 0x7f45092dd350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TemporalBlock.call of <__main__.TemporalBlock object at 0x7f45092dd350>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "Modified inputs:  (1, 48, 256) original:  (1, 45, 256)\n",
            "WARNING:tensorflow:Entity <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f45045bf3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f45045bf3d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f45045bf3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f45045bf3d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f450930dc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f450930dc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f450930dc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f450930dc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "X1 shape:  (1, 12, 256)\n",
            "WARNING:tensorflow:Entity <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f4509384e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f4509384e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f4509384e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f4509384e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <__main__.LayerNormalization object at 0x7f450930d850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <__main__.LayerNormalization object at 0x7f450930d850>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <__main__.LayerNormalization object at 0x7f450930d850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <__main__.LayerNormalization object at 0x7f450930d850>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "Step 1 mean:  (1, 2)\n",
            "Step 1 mean:  (1, 2, 256)\n",
            "Step 2 variance:  (1, 2, 1)\n",
            "Vineet: (1, 2, 256)\n",
            "TF layer norm: (1, 2, 256)\n",
            "\n",
            "\n",
            "\n",
            "<class 'numpy.float32'> <class 'numpy.float32'>\n",
            "[ 0.69518757  1.3975507  -0.649979   -0.00726203 -0.5186878  -0.649979\n",
            "  1.8926712   3.1499934   2.7983959   1.098325   -0.649979   -0.649979\n",
            " -0.649979   -0.38057926 -0.649979   -0.649979  ]\n",
            "\n",
            "\n",
            "\n",
            "[ 0.67207235  1.3510818  -0.6283671  -0.00702065 -0.5014414  -0.6283671\n",
            "  1.8297393   3.0452554   2.7053485   1.0618055  -0.6283671  -0.6283671\n",
            " -0.6283671  -0.367925   -0.6283671  -0.6283671 ]\n",
            "\n",
            "\n",
            "\n",
            "[-0.64640695  1.324365   -0.64640695  0.7535492  -0.64640695 -0.64640695\n",
            " -0.64640695  0.9410761   0.22900899  0.3485515  -0.45108992 -0.64640695\n",
            "  0.47109133  0.86711794 -0.63364714 -0.64640695]\n",
            "\n",
            "\n",
            "\n",
            "[-0.6283671   1.4044166  -0.6283671   0.8156397  -0.6283671  -0.6283671\n",
            " -0.6283671   1.0090673   0.27459443  0.3978985  -0.42690432 -0.6283671\n",
            "  0.5242941   0.932782   -0.6152059  -0.6283671 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOj0ysYfL3rS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a521e818-0f41-42a1-e51e-7489ba883af3"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default() as g:\n",
        "    x = tf.random_normal((10, 45, 256)) # (batch_size, length, channel)\n",
        "    # n_outputs, kernel_size, strides, dilation_rate\n",
        "    tblock = TemporalBlock(n_outputs=256, kernel_size=2, strides=1, dilation_rate=1)\n",
        "    output = tblock(x, training=tf.constant(True))\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "    sess.run(init)\n",
        "    res = sess.run(output)\n",
        "    print(res.shape)\n",
        "    print(res)  \n",
        "    print(res[0, :, 0])\n",
        "    # print(res[1, :, 1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method TemporalBlock.call of <__main__.TemporalBlock object at 0x7f4509476610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TemporalBlock.call of <__main__.TemporalBlock object at 0x7f4509476610>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TemporalBlock.call of <__main__.TemporalBlock object at 0x7f4509476610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TemporalBlock.call of <__main__.TemporalBlock object at 0x7f4509476610>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "Modified inputs:  (10, 48, 256) original:  (10, 45, 256)\n",
            "WARNING:tensorflow:Entity <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f4509406490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f4509406490>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f4509406490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f4509406490>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f45094f1590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f45094f1590>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f45094f1590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f45094f1590>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "X1 shape:  (10, 12, 256)\n",
            "WARNING:tensorflow:Entity <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f4509340950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f4509340950>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f4509340950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7f4509340950>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <__main__.LayerNormalization object at 0x7f45093d3dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <__main__.LayerNormalization object at 0x7f45093d3dd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method LayerNormalization.call of <__main__.LayerNormalization object at 0x7f45093d3dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <__main__.LayerNormalization object at 0x7f45093d3dd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "(10, 2, 256)\n",
            "[[[ 1.076151    2.4806857   0.3438147  ...  0.78642607  0.77645195\n",
            "   -0.3247258 ]\n",
            "  [-0.01145346  0.71144634 -0.07523614 ... -0.6552048  -0.6552048\n",
            "   -0.6552048 ]]\n",
            "\n",
            " [[ 2.142585    0.64131314 -0.54366225 ... -0.7358025  -0.5633334\n",
            "   -0.53629136]\n",
            "  [-0.66515595  0.9099496   1.4428179  ... -0.66515595 -0.10320154\n",
            "   -0.66515595]]\n",
            "\n",
            " [[-0.6768589   0.02260999 -0.6768589  ...  2.1301756  -0.6768589\n",
            "   -0.6768589 ]\n",
            "  [-0.7614702   1.0422966  -0.28541365 ... -0.09996591  0.5962766\n",
            "   -0.43815005]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.6870708  -0.63675857  0.9309146  ...  1.2461637  -0.63675857\n",
            "    1.4270027 ]\n",
            "  [ 0.43117163 -0.5559139   4.4189005  ... -0.6597238  -0.6597238\n",
            "   -0.6597238 ]]\n",
            "\n",
            " [[-0.1445648  -0.64350545  0.95935935 ... -0.64350545 -0.6288977\n",
            "   -0.64350545]\n",
            "  [-0.1837035   1.0011969  -0.69185734 ... -0.69185734 -0.69185734\n",
            "   -0.69185734]]\n",
            "\n",
            " [[-0.64604884  0.48686978 -0.586327   ... -0.64604884 -0.64604884\n",
            "   -0.64604884]\n",
            "  [ 0.49726933 -0.67756504  0.49468592 ... -0.67756504 -0.57987547\n",
            "   -0.67756504]]]\n",
            "[ 1.076151   -0.01145346]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9pz2h62ZFrj"
      },
      "source": [
        "shift_len > kernel_size_1\n",
        "first block size = 2 * shift"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TbQeHkslq5V"
      },
      "source": [
        "Source TCN output 0 - 32 = Espresso TCN output 0-32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVMGHgY9lwjF"
      },
      "source": [
        "Source TCN output 16 - 48 = Espresso TCN output 16-48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf1kTEtHl2M4"
      },
      "source": [
        "Source TCN output 0 - 48 = Espresso TCN output 1 (0-32) (16-48) doesn't match"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgJstJfXAetJ"
      },
      "source": [
        "# tf.reset_default_graph()\n",
        "# # tf.random.set_seed(5)\n",
        "# with tf.Graph().as_default() as g:\n",
        "#     x = tf.random_normal((1, 48, 256), seed=2)[:] # (batch_size, length, channel)\n",
        "#     print(x[0, :, 0])\n",
        "#     # n_outputs, kernel_size, strides, dilation_rate\n",
        "#     tblock = TemporalBlock(n_outputs=256, kernel_size=2, strides=1, dilation_rate=1)\n",
        "#     output = tblock(i, training=tf.constant(True))\n",
        "#     init = tf.global_variables_initializer()\n",
        "    \n",
        "    \n",
        "# with tf.Session(graph=g) as sess:\n",
        "#     sess.run(init)\n",
        "#     res = sess.run(output)\n",
        "#     print(res.shape)   \n",
        "#     print(res[0, :, 0])\n",
        "#     # print(res[1, :, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3il-_oBfqH9N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "outputId": "b2e022d6-fa5c-4ec1-867c-ea372837bc57"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default() as g:\n",
        "    x = tf.random_normal((1, 48, 256), seed=2) # (batch_size, length, channel)\n",
        "    LayerNorm = LayerNormalization()\n",
        "    x = LayerNorm(x)\n",
        "    print(x)\n",
        "    # x = tf.identity(i)\n",
        "    # n_outputs, kernel_size, strides, dilation_rate\n",
        "    # tblock = TemporalBlock(n_outputs=256, kernel_size=2, strides=1, dilation_rate=1)\n",
        "    # output = tblock(x, training=tf.constant(True))\n",
        "    # init = tf.global_variables_initializer()\n",
        "    \n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "    sess.run(init)\n",
        "    res = sess.run(output)\n",
        "    print(res.shape)   \n",
        "    print(res[0, :, 0])\n",
        "    # print(res[1, :, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method LayerNormalization.call of <__main__.LayerNormalization object at 0x7fa1a8c3ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <__main__.LayerNormalization object at 0x7fa1a8c3ded0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LayerNormalization.call of <__main__.LayerNormalization object at 0x7fa1a8c3ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <__main__.LayerNormalization object at 0x7fa1a8c3ded0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "Step 1 mean:  (1, 48)\n",
            "Step 3 mean:  (1, 48)\n",
            "Tensor(\"layer_normalization/add_1:0\", shape=(1, 48, 256), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    302\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 303\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    304\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3795\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3796\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3879\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3880\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Operation %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3881\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Operation name: \"init\"\nop: \"NoOp\"\ninput: \"^temporal_block/conv1/kernel/Assign\"\ninput: \"^temporal_block/conv1/bias/Assign\"\ninput: \"^temporal_block/conv2/kernel/Assign\"\ninput: \"^temporal_block/conv2/bias/Assign\"\ninput: \"^temporal_block/layer_normalization/layer_norm_scale/Assign\"\ninput: \"^temporal_block/layer_normalization/layer_norm_bias/Assign\"\n is not an element of this graph.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-196-2b5e4b05cffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1158\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \"\"\"\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[0;32m--> 310\u001b[0;31m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
            "\u001b[0;31mValueError\u001b[0m: Fetch argument <tf.Operation 'init' type=NoOp> cannot be interpreted as a Tensor. (Operation name: \"init\"\nop: \"NoOp\"\ninput: \"^temporal_block/conv1/kernel/Assign\"\ninput: \"^temporal_block/conv1/bias/Assign\"\ninput: \"^temporal_block/conv2/kernel/Assign\"\ninput: \"^temporal_block/conv2/bias/Assign\"\ninput: \"^temporal_block/layer_normalization/layer_norm_scale/Assign\"\ninput: \"^temporal_block/layer_normalization/layer_norm_bias/Assign\"\n is not an element of this graph.)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V54OTp8SSELu",
        "outputId": "64a56e94-a86a-4449-a977-73d788db66da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def cum_mean(arr):\n",
        "    cum_sum = np.cumsum(arr, axis=0, dtype=float)    \n",
        "    for i in range(cum_sum.shape[0]):       \n",
        "        if i == 0:\n",
        "            continue        \n",
        "        # print(cum_sum[i] / (i + 1))\n",
        "        cum_sum[i] =  cum_sum[i] / (i + 1)\n",
        "    return cum_sum\n",
        "epsilon=1e-6\n",
        "x = np.random.random((1, 6, 2))\n",
        "step1_mean = np.mean(x, axis=2)\n",
        "step1_mean = step1_mean.flatten()\n",
        "\n",
        "mean = cum_mean(step1_mean)\n",
        "# repeat\n",
        "mean = np.expand_dims(mean, axis=[0,2])\n",
        "mean = np.tile(mean, x.shape[2])\n",
        "\n",
        "print(x - mean)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 0.23122015 -0.23122015]\n",
            "  [ 0.10669028  0.04515204]\n",
            "  [-0.15216137 -0.10007275]\n",
            "  [ 0.47764491  0.42627682]\n",
            "  [-0.07139401 -0.36781364]\n",
            "  [-0.27442334  0.29475225]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjyrTFbz9yiY",
        "outputId": "dcaf59d8-8dc2-4907-f901-5a01e2896a4c"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default() as g:\n",
        "    x = tf.random_normal((2, 47, 256)) # (batch_size, length, channel)\n",
        "    # n_outputs, kernel_size, strides, dilation_rate\n",
        "    tblock = TemporalBlock(n_outputs=256, kernel_size=2, strides=1, dilation_rate=1)\n",
        "    output = tblock(x, training=tf.constant(True))\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "    sess.run(init)\n",
        "    res = sess.run(output)\n",
        "    print(res.shape)   \n",
        "    print(res[0, :, 0])\n",
        "    print(res[1, :, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method TemporalBlock.call of <__main__.TemporalBlock object at 0x7fcf13ebfed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TemporalBlock.call of <__main__.TemporalBlock object at 0x7fcf13ebfed0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method TemporalBlock.call of <__main__.TemporalBlock object at 0x7fcf13ebfed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TemporalBlock.call of <__main__.TemporalBlock object at 0x7fcf13ebfed0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "16\n",
            "Modified inputs:  (2, 48, 256) original:  (2, 47, 256)\n",
            "WARNING:tensorflow:Entity <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7fcf13ebff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7fcf13ebff10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7fcf13ebff10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7fcf13ebff10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fcf18dbef10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fcf18dbef10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fcf18dbef10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fcf18dbef10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "X1 shape:  (2, 8, 256)\n",
            "WARNING:tensorflow:Entity <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7fcf13ebfd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7fcf13ebfd90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7fcf13ebfd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method CausalConv1D.call of <__main__.CausalConv1D object at 0x7fcf13ebfd90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fcf24329a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fcf24329a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fcf24329a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7fcf24329a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "X2 shape:  (2, 2, 256)\n",
            "(2, 48, 256) (2, 32, 256)\n",
            "(2, 47, 256)\n",
            "[0.0000000e+00 0.0000000e+00 1.7744056e+00 9.0063453e-01 1.2888507e+00\n",
            " 6.2661827e-01 9.1972822e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 1.7668011e-03 1.7425556e+00 0.0000000e+00 7.1024650e-01\n",
            " 8.6270563e-02 1.9126055e+00 6.6705036e-01 1.0853175e-01 0.0000000e+00\n",
            " 1.9394204e-01 3.1168628e+00 6.4455308e-02 2.2087263e-01 0.0000000e+00\n",
            " 0.0000000e+00 0.0000000e+00 7.3721683e-01 6.9630042e-02 9.0151519e-01\n",
            " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 2.0612460e-02 0.0000000e+00 0.0000000e+00 7.9387873e-01\n",
            " 0.0000000e+00 3.7305686e-01 6.3145775e-01 0.0000000e+00 0.0000000e+00\n",
            " 3.5856661e-01 0.0000000e+00]\n",
            "[0.7978143  0.         1.0751885  1.1837944  0.13714105 0.33656406\n",
            " 1.3389802  0.         0.         1.4031792  1.4371414  1.9136165\n",
            " 0.36016485 0.         1.2857449  0.         0.         0.\n",
            " 0.06822962 0.         1.4316689  0.         1.8964096  0.\n",
            " 0.         0.         0.         0.         0.         0.9256435\n",
            " 0.         1.2422333  0.63086206 1.1543427  1.295483   0.\n",
            " 0.         0.         1.4663275  0.18445602 0.5907238  0.69171876\n",
            " 0.         1.6423852  1.7289053  0.         0.9798911 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6DAGeumN4ko"
      },
      "source": [
        "## END\n",
        "\n",
        "## Experiment more below:\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IZtIPiMaZ0b",
        "outputId": "358412d6-2d49-4853-a6ff-f3d7769c4d78"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default() as g:\n",
        "    x = tf.random_normal((32, 8, 256)) # (batch_size, length, channel)\n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    N = tf.constant(8)\n",
        "    # M = 3\n",
        "    K = x.shape[1] # for here 3\n",
        "    order = tf.range(0, N*K, K)\n",
        "    K_array = tf.range(0, K)\n",
        "    # print('Order: ', sess.run(order))\n",
        "    # print('K: ', sess.run(K_array))\n",
        "\n",
        "    x_ = tf.expand_dims(order, 0)\n",
        "    y_ = tf.expand_dims(K_array, 1)\n",
        "    # print(x_.shape, y_.shape, tf.add(x_, y_).shape)\n",
        "    z = tf.reshape(tf.add(x_, y_), [-1, N])\n",
        "    print('Sum: ', sess.run(z), z.shape)\n",
        "    indices = tf.reshape(z, [-1])\n",
        "    print(sess.run(indices), indices.shape)\n",
        "\n",
        "    #\n",
        "    # For checking purposes\n",
        "    #\n",
        "    # order = list(range(0, M*K, K))\n",
        "    # order = [[x+i for x in order] for i in range(K)]\n",
        "    # order = list(itertools.chain.from_iterable(order))\n",
        "    # print('Should be', order)\n",
        "    # x_rep = tf.gather(tf.tile(x, [1, N, 1]), indices, axis=1)\n",
        "    # print(x_rep.shape)\n",
        "\n",
        "    s = tf.reduce_sum(x, axis=(0, 2))\n",
        "    s = sess.run(s)\n",
        "    print('Sum original: ', s.shape, s)\n",
        "\n",
        "    # s = tf.reduce_sum(x_rep, axis=(0, 2))\n",
        "    # s = sess.run(s)\n",
        "    # print('Sum original: ', s.shape, s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sum:  [[ 0  8 16 24 32 40 48 56]\n",
            " [ 1  9 17 25 33 41 49 57]\n",
            " [ 2 10 18 26 34 42 50 58]\n",
            " [ 3 11 19 27 35 43 51 59]\n",
            " [ 4 12 20 28 36 44 52 60]\n",
            " [ 5 13 21 29 37 45 53 61]\n",
            " [ 6 14 22 30 38 46 54 62]\n",
            " [ 7 15 23 31 39 47 55 63]] (8, 8)\n",
            "[ 0  8 16 24 32 40 48 56  1  9 17 25 33 41 49 57  2 10 18 26 34 42 50 58\n",
            "  3 11 19 27 35 43 51 59  4 12 20 28 36 44 52 60  5 13 21 29 37 45 53 61\n",
            "  6 14 22 30 38 46 54 62  7 15 23 31 39 47 55 63] (64,)\n",
            "Sum original:  (8,) [ 47.629395 -35.18349   46.662994 -68.17631   80.89191  175.77397\n",
            " 117.67242   55.33221 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Muaq4g9yk5"
      },
      "source": [
        "class TemporalConvNet(tf.layers.Layer):\n",
        "    def __init__(self, num_channels, kernel_size=2, dropout=0.2,\n",
        "                 trainable=True, name=None, dtype=None, \n",
        "                 activity_regularizer=None, **kwargs):\n",
        "        super(TemporalConvNet, self).__init__(\n",
        "            trainable=trainable, dtype=dtype,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            name=name, **kwargs\n",
        "        )\n",
        "        self.layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            out_channels = num_channels[i]\n",
        "            self.layers.append(\n",
        "                TemporalBlock(out_channels, kernel_size, strides=1, dilation_rate=dilation_size,\n",
        "                              dropout=dropout, name=\"tblock_{}\".format(i))\n",
        "            )\n",
        "    \n",
        "    def call(self, inputs, training=True):\n",
        "        outputs = inputs\n",
        "        for layer in self.layers:\n",
        "            outputs = layer(outputs, training=training)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default() as g:\n",
        "    x = tf.random_normal((32, 128, 280)) # (batch_size, length, channel)\n",
        "    tcn = TemporalConvNet([8, 8, 8, 8], 2, 0.25)\n",
        "    output = tcn(x, training=tf.constant(True))\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    res = sess.run(output)\n",
        "    print(res.shape)   \n",
        "    print(res[0, :, 0])\n",
        "    print(res[1, :, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOLMXnCm9ywZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH2hZs9W9yzV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_x00hIey-fw"
      },
      "source": [
        "## Building TCNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcFQu3F0y-fy"
      },
      "source": [
        "###  Causal Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u-dyRIPy-fy"
      },
      "source": [
        "class CausalConv1D(tf.layers.Conv1D):\n",
        "    def __init__(self, filters,\n",
        "               kernel_size,\n",
        "               strides=1,\n",
        "               dilation_rate=1,\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               kernel_initializer=None,\n",
        "               bias_initializer=tf.zeros_initializer(),\n",
        "               kernel_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               trainable=True,\n",
        "               name=None,\n",
        "               **kwargs):\n",
        "        super(CausalConv1D, self).__init__(\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=strides,\n",
        "            padding='valid',\n",
        "            data_format='channels_last',\n",
        "            dilation_rate=dilation_rate,\n",
        "            activation=activation,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            kernel_constraint=kernel_constraint,\n",
        "            bias_constraint=bias_constraint,\n",
        "            trainable=trainable,\n",
        "            name=name, **kwargs\n",
        "        )\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        padding = (self.kernel_size[0] - 1) * self.dilation_rate[0]\n",
        "        if self.data_format == 'channels_first':\n",
        "            inputs = tf.pad(inputs, tf.constant([[0, 0], [0, 0], [padding, 0]], dtype=tf.int32))\n",
        "        else:\n",
        "            inputs = tf.pad(inputs, tf.constant([(0, 0,), (padding, 0), (0, 0)]))\n",
        "        return super(CausalConv1D, self).call(inputs), inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FIx5P1ty-f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "11417725-57d2-47cf-b4a1-948527867f23"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default() as g:\n",
        "    x = tf.random_normal((32, 10, 4)) # (batch_size, length, channel)\n",
        "    with tf.variable_scope(\"tcn\"):\n",
        "        conv = CausalConv1D(8, 3, activation=tf.nn.relu)\n",
        "    output = conv(x)\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    res, inputs = sess.run(output)\n",
        "    print(inputs.shape)\n",
        "    print(inputs[0, :, 0])\n",
        "    print(res.shape)    \n",
        "    print(res[0, :, 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 12, 4)\n",
            "[ 0.          0.          0.88044167 -1.2032915   0.29814827 -1.1900542\n",
            "  1.0388981  -0.07884882 -1.2475842  -0.48100722 -1.7068204  -1.0657506 ]\n",
            "(32, 10, 8)\n",
            "[0.         0.         1.2657795  1.5949045  0.         0.\n",
            " 0.         0.         0.27693093 0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CYDJDCGy-f-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0e700a07-5b49-46f2-a088-6cff89548eb5"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default() as g:\n",
        "    x = tf.expand_dims(\n",
        "        tf.expand_dims(tf.constant([1, 0, 0, 1, 0, 0, 1], dtype=tf.float32), axis=0),\n",
        "        axis=-1) # (batch_size, length, channel)\n",
        "    with tf.variable_scope(\"tcn\"):\n",
        "        conv = CausalConv1D(8, 2, dilation_rate=2, activation=None)\n",
        "    output = conv(x)\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    res, inputs = sess.run(output)\n",
        "    print(inputs.shape)\n",
        "    print(inputs[0, :, 0])\n",
        "    print(res.shape)    \n",
        "    print(res[0, :, 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 9, 1)\n",
            "[0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
            "(1, 7, 8)\n",
            "[0.1447475  0.         0.48786867 0.1447475  0.         0.48786867\n",
            " 0.1447475 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByuyggHey-gI"
      },
      "source": [
        "###  Spatial Dropout\n",
        "\n",
        "Reference: https://stats.stackexchange.com/questions/282282/how-is-spatial-dropout-in-2d-implemented\n",
        "\n",
        "Actually, simply setting noise_shape in tf.layers.Dropout will do the trick."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRTsgwSGy-gK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "aa51c39b-e40f-4769-c4f7-f42c9bd9862d"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default() as g:\n",
        "    x = tf.random_normal((32, 4, 10)) # (batch_size, channel, length)\n",
        "    dropout = tf.layers.Dropout(0.5, noise_shape=[x.shape[0], x.shape[1], tf.constant(1)])\n",
        "    output = dropout(x, training=True)\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    res = sess.run(output)\n",
        "    print(res.shape)   \n",
        "    print(res[0, :, :])\n",
        "    print(res[1, :, :])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 4, 10)\n",
            "[[ 0.          0.         -0.          0.          0.         -0.\n",
            "  -0.         -0.          0.         -0.        ]\n",
            " [-1.3023647  -1.628788    0.20181039  1.554159    5.3209696   1.7473009\n",
            "  -0.41964796 -0.6231473  -2.0326247  -0.21218014]\n",
            " [ 1.1885451  -1.3423481  -1.3000014  -1.132894   -0.20258099 -1.6488353\n",
            "   1.3672652   3.4905746  -0.01186325 -0.8049923 ]\n",
            " [ 0.          0.          0.         -0.         -0.          0.\n",
            "   0.          0.          0.         -0.        ]]\n",
            "[[ 0.8047661  -2.3998349   0.68522704  0.7751469  -0.6081628  -4.7214503\n",
            "  -1.3095977   0.8691299  -2.2773757  -0.2609347 ]\n",
            " [-0.          0.          0.         -0.          0.         -0.\n",
            "   0.         -0.         -0.          0.        ]\n",
            " [ 0.         -0.          0.          0.         -0.          0.\n",
            "   0.         -0.          0.         -0.        ]\n",
            " [ 0.         -0.         -0.          0.          0.         -0.\n",
            "   0.          0.         -0.          0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdTffJqQy-gU"
      },
      "source": [
        "### Temporal blocks\n",
        "\n",
        "Note: `tf.contrib.layers.layer_norm` only supports `channels_last`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bHYwhRxy-gW"
      },
      "source": [
        "# Redefining CausalConv1D to simplify its return values\n",
        "class CausalConv1D(tf.layers.Conv1D):\n",
        "    def __init__(self, filters,\n",
        "               kernel_size,\n",
        "               strides=1,\n",
        "               dilation_rate=1,\n",
        "               activation=None,\n",
        "               use_bias=True,\n",
        "               kernel_initializer=None,\n",
        "               bias_initializer=tf.zeros_initializer(),\n",
        "               kernel_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               trainable=True,\n",
        "               name=None,\n",
        "               **kwargs):\n",
        "        super(CausalConv1D, self).__init__(\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=strides,\n",
        "            padding='valid',\n",
        "            data_format='channels_last',\n",
        "            dilation_rate=dilation_rate,\n",
        "            activation=activation,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            kernel_constraint=kernel_constraint,\n",
        "            bias_constraint=bias_constraint,\n",
        "            trainable=trainable,\n",
        "            name=name, **kwargs\n",
        "        )\n",
        "       \n",
        "    def call(self, inputs):\n",
        "        padding = (self.kernel_size[0] - 1) * self.dilation_rate[0]\n",
        "        inputs = tf.pad(inputs, tf.constant([(0, 0,), (1, 0), (0, 0)]) * padding)\n",
        "        return super(CausalConv1D, self).call(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re2siwPgy-ga"
      },
      "source": [
        "class TemporalBlock(tf.layers.Layer):\n",
        "    def __init__(self, n_outputs, kernel_size, strides, dilation_rate, dropout=0.2, \n",
        "                 trainable=True, name=None, dtype=None, \n",
        "                 activity_regularizer=None, **kwargs):\n",
        "        super(TemporalBlock, self).__init__(\n",
        "            trainable=trainable, dtype=dtype,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            name=name, **kwargs\n",
        "        )        \n",
        "        self.dropout = dropout\n",
        "        self.n_outputs = n_outputs\n",
        "        self.conv1 = CausalConv1D(\n",
        "            n_outputs, kernel_size, strides=strides, \n",
        "            dilation_rate=dilation_rate, activation=tf.nn.relu, \n",
        "            name=\"conv1\")\n",
        "        self.conv2 = CausalConv1D(\n",
        "            n_outputs, kernel_size, strides=strides, \n",
        "            dilation_rate=dilation_rate, activation=tf.nn.relu, \n",
        "            name=\"conv2\")\n",
        "        self.down_sample = None\n",
        "\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        channel_dim = 2\n",
        "        self.dropout1 = tf.layers.Dropout(self.dropout, [tf.constant(1), tf.constant(1), tf.constant(self.n_outputs)])\n",
        "        self.dropout2 = tf.layers.Dropout(self.dropout, [tf.constant(1), tf.constant(1), tf.constant(self.n_outputs)])\n",
        "        if input_shape[channel_dim] != self.n_outputs:\n",
        "            # self.down_sample = tf.layers.Conv1D(\n",
        "            #     self.n_outputs, kernel_size=1, \n",
        "            #     activation=None, data_format=\"channels_last\", padding=\"valid\")\n",
        "            self.down_sample = tf.layers.Dense(self.n_outputs, activation=None)\n",
        "        self.built = True\n",
        "    \n",
        "    def call(self, inputs, training=True):\n",
        "        x = self.conv1(inputs)\n",
        "        x = tf.contrib.layers.layer_norm(x)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.conv2(x)\n",
        "        x = tf.contrib.layers.layer_norm(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        if self.down_sample is not None:\n",
        "            inputs = self.down_sample(inputs)\n",
        "        return tf.nn.relu(x + inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CMZKL1jy-ge",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9b76c2fb-130a-4255-dbb2-65ad7960ba6a"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default() as g:\n",
        "    x = tf.random_normal((32, 10, 4)) # (batch_size, length, channel)\n",
        "    tblock = TemporalBlock(8, 2, 1, 1)\n",
        "    output = tblock(x, training=tf.constant(True))\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    res = sess.run(output)\n",
        "    print(res.shape)   \n",
        "    print(res[0, :, 0])\n",
        "    print(res[1, :, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 10, 8)\n",
            "[0.         1.534969   0.         1.4863     2.3868496  0.9489206\n",
            " 0.         0.42759717 0.         0.        ]\n",
            "[0.04622638 0.         3.0465143  0.350043   0.         0.42763186\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhzHzZtMy-gk"
      },
      "source": [
        "### Temporal convolutional networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeLztD1Ly-gm"
      },
      "source": [
        "class TemporalConvNet(tf.layers.Layer):\n",
        "    def __init__(self, num_channels, kernel_size=2, dropout=0.2,\n",
        "                 trainable=True, name=None, dtype=None, \n",
        "                 activity_regularizer=None, **kwargs):\n",
        "        super(TemporalConvNet, self).__init__(\n",
        "            trainable=trainable, dtype=dtype,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            name=name, **kwargs\n",
        "        )\n",
        "        self.layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            out_channels = num_channels[i]\n",
        "            self.layers.append(\n",
        "                TemporalBlock(out_channels, kernel_size, strides=1, dilation_rate=dilation_size,\n",
        "                              dropout=dropout, name=\"tblock_{}\".format(i))\n",
        "            )\n",
        "    \n",
        "    def call(self, inputs, training=True):\n",
        "        outputs = inputs\n",
        "        for layer in self.layers:\n",
        "            outputs = layer(outputs, training=training)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRTtl0mey-go",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c8ca7755-35ea-4a41-9ef7-5770643f7b8b"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default() as g:\n",
        "    x = tf.random_normal((32, 10, 4)) # (batch_size, length, channel)\n",
        "    tcn = TemporalConvNet([8, 8, 8, 8], 2, 0.25)\n",
        "    output = tcn(x, training=tf.constant(True))\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    res = sess.run(output)\n",
        "    print(res.shape)   \n",
        "    print(res[0, :, 0])\n",
        "    print(res[1, :, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 10, 8)\n",
            "[0.6988858  0.         0.7061405  0.15213768 0.93309164 0.\n",
            " 1.3751144  2.6659122  1.8665429  0.        ]\n",
            "[0.         0.         4.748131   0.24300182 0.         0.\n",
            " 0.         0.         1.4196995  0.8674514 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCMhWfWjy-g0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "121c2c0f-7066-4f57-fe2c-9d29a8606c6b"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "    Xinput = tf.placeholder(tf.float32, shape=[None, 10, 4])\n",
        "    tcn = TemporalConvNet([8, 8, 8, 8], 2, 0.25)\n",
        "    output = tcn(Xinput, training=tf.constant(True))\n",
        "    print(tcn.layers[0].down_sample)    \n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    res = sess.run(output, {Xinput: np.random.randn(32, 10, 4)})\n",
        "    print(res.shape)   \n",
        "    print(res[0, :, 0])\n",
        "    print(res[1, :, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.layers.core.Dense object at 0x7fddee04f940>\n",
            "(32, 10, 8)\n",
            "[0.        0.        2.1618829 0.        3.862938  2.824544  4.999326\n",
            " 1.7338551 6.2948    1.4054765]\n",
            "[0.        0.        0.        0.        0.        0.        0.\n",
            " 7.947054  5.4993486 0.       ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jYugVyby-g-"
      },
      "source": [
        "## Sequential MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41qAk9lAy-hC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2f114c1-f9b7-401d-99be-f240426bed49"
      },
      "source": [
        "# Training Parameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "display_step = 500\n",
        "total_batch = int(mnist.train.num_examples / batch_size)\n",
        "print(\"Number of batches per epoch:\", total_batch)\n",
        "training_steps = 3000\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 1 # MNIST data input (img shape: 28*28)\n",
        "timesteps = 28 * 28 # timesteps\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)\n",
        "dropout = 0.1\n",
        "kernel_size = 8\n",
        "levels = 6\n",
        "nhid = 20 # hidden layer num of features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of batches per epoch: 859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP37UtN5y-hG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "089539a5-b6c1-4cb0-ca1c-deacde5b3cbe"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    tf.set_random_seed(10)\n",
        "    # tf Graph input\n",
        "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
        "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
        "    is_training = tf.placeholder(\"bool\")\n",
        "    \n",
        "    # Define weights\n",
        "    logits = tf.layers.dense(\n",
        "        TemporalConvNet([nhid] * levels, kernel_size, dropout)(\n",
        "            X, training=is_training)[:, -1, :],\n",
        "        num_classes, activation=None, \n",
        "        kernel_initializer=tf.orthogonal_initializer()\n",
        "    )\n",
        "    prediction = tf.nn.softmax(logits)\n",
        "   \n",
        "    # Define loss and optimizer\n",
        "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        logits=logits, labels=Y))\n",
        "    \n",
        "    with tf.name_scope(\"optimizer\"):\n",
        "        # optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        # gvs = optimizer.compute_gradients(loss_op)\n",
        "        # for grad, var in gvs:\n",
        "        #     if grad is None:\n",
        "        #         print(var)\n",
        "        # capped_gvs = [(tf.clip_by_value(grad, -.5, .5), var) for grad, var in gvs]\n",
        "        # train_op = optimizer.apply_gradients(capped_gvs)    \n",
        "        train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "    # Evaluate model (with test logits, for dropout to be disabled)\n",
        "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "    # Initialize the variables (i.e. assign their default value)\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
        "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All parameters: 108992.0\n",
            "Trainable parameters: 36330\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjwOnIUmy-hM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "79b8a85e-9f56-458d-ecc9-3eea13094548"
      },
      "source": [
        "# Start training\n",
        "log_dir = \"logs/tcn/%s\" % datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
        "tb_writer = tf.summary.FileWriter(log_dir, graph)\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = False\n",
        "best_val_acc = 0.8\n",
        "with tf.Session(graph=graph, config=config) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    for step in range(1, training_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # print(np.max(batch_x), np.mean(batch_x), np.median(batch_x))\n",
        "        # Reshape data to get 28 * 28 seq of 1 elements\n",
        "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, is_training: True})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={\n",
        "                X: batch_x, Y: batch_y, is_training: False})\n",
        "            # Calculate accuracy for 128 mnist test images\n",
        "            test_len = 128\n",
        "            test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
        "            test_label = mnist.test.labels[:test_len]\n",
        "            val_acc = sess.run(accuracy, feed_dict={X: test_data, Y: test_label, is_training: False})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc) + \", Test Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(val_acc))\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
        "                print(\"Model saved in path: %s\" % save_path)\n",
        "    print(\"Optimization Finished!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 3.6770, Training Accuracy= 0.109, Test Accuracy= 0.156\n",
            "Step 500, Minibatch Loss= 0.1022, Training Accuracy= 0.953, Test Accuracy= 0.945\n",
            "Model saved in path: /tmp/model.ckpt\n",
            "Step 1000, Minibatch Loss= 0.2515, Training Accuracy= 0.922, Test Accuracy= 0.992\n",
            "Model saved in path: /tmp/model.ckpt\n",
            "Step 1500, Minibatch Loss= 0.0310, Training Accuracy= 0.984, Test Accuracy= 0.992\n",
            "Step 2000, Minibatch Loss= 0.1406, Training Accuracy= 0.953, Test Accuracy= 0.984\n",
            "Step 2500, Minibatch Loss= 0.0131, Training Accuracy= 1.000, Test Accuracy= 0.984\n",
            "Step 3000, Minibatch Loss= 0.0228, Training Accuracy= 1.000, Test Accuracy= 0.984\n",
            "Step 3500, Minibatch Loss= 0.1202, Training Accuracy= 0.969, Test Accuracy= 1.000\n",
            "Model saved in path: /tmp/model.ckpt\n",
            "Step 4000, Minibatch Loss= 0.0847, Training Accuracy= 0.984, Test Accuracy= 1.000\n",
            "Step 4500, Minibatch Loss= 0.0906, Training Accuracy= 0.953, Test Accuracy= 0.992\n",
            "Step 5000, Minibatch Loss= 0.0346, Training Accuracy= 0.984, Test Accuracy= 1.000\n",
            "Optimization Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91elDQ4Xy-hS"
      },
      "source": [
        "## Permuted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMtyE5m-97FK"
      },
      "source": [
        "training_steps = 5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri6zKrm9Ie30"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWX3IRfR0z5O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "800eba07-4ffc-492d-d5fc-7906308a9f51"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    tf.set_random_seed(10)\n",
        "    # tf Graph input\n",
        "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
        "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
        "    is_training = tf.placeholder(\"bool\")\n",
        "    \n",
        "    # Permute the time step\n",
        "    np.random.seed(100)\n",
        "    permute = np.random.permutation(784)\n",
        "    X_shuffled = tf.gather(X, permute, axis=1)\n",
        "    \n",
        "    # Define weights\n",
        "    logits = tf.layers.dense(\n",
        "        TemporalConvNet([nhid] * levels, kernel_size, dropout)(\n",
        "            X_shuffled, training=is_training)[:, -1, :],\n",
        "        num_classes, activation=None, \n",
        "        kernel_initializer=tf.orthogonal_initializer()\n",
        "    )\n",
        "    prediction = tf.nn.softmax(logits)\n",
        "   \n",
        "    # Define loss and optimizer\n",
        "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        logits=logits, labels=Y))\n",
        "    \n",
        "    with tf.name_scope(\"optimizer\"):\n",
        "        # optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        # gvs = optimizer.compute_gradients(loss_op)\n",
        "        # for grad, var in gvs:\n",
        "        #     if grad is None:\n",
        "        #         print(var)\n",
        "        # capped_gvs = [(tf.clip_by_value(grad, -.5, .5), var) for grad, var in gvs]\n",
        "        # train_op = optimizer.apply_gradients(capped_gvs)    \n",
        "        train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "    # Evaluate model (with test logits, for dropout to be disabled)\n",
        "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "    # Initialize the variables (i.e. assign their default value)\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    print(\"All parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.global_variables()]))\n",
        "    print(\"Trainable parameters:\", np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All parameters: 108992.0\n",
            "Trainable parameters: 36330\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oprHW-Qry-ha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "7f544e34-7cb8-44fb-ff2e-8ad3ee669d6f"
      },
      "source": [
        "# Start training\n",
        "log_dir = \"logs/tcn/%s\" % datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "Path(log_dir).mkdir(exist_ok=True, parents=True)\n",
        "tb_writer = tf.summary.FileWriter(log_dir, graph)\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "best_val_acc = 0.8\n",
        "with tf.Session(graph=graph, config=config) as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    for step in range(1, training_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # print(np.max(batch_x), np.mean(batch_x), np.median(batch_x))\n",
        "        # Reshape data to get 28 * 28 seq of 1 elements\n",
        "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, is_training: True})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={\n",
        "                X: batch_x, Y: batch_y, is_training: False})\n",
        "            # Calculate accuracy for 128 mnist test images\n",
        "            test_len = 128\n",
        "            test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
        "            test_label = mnist.test.labels[:test_len]\n",
        "            val_acc = sess.run(accuracy, feed_dict={X: test_data, Y: test_label, is_training: False})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc) + \", Test Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(val_acc))\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
        "                print(\"Model saved in path: %s\" % save_path)\n",
        "    print(\"Optimization Finished!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 3.7196, Training Accuracy= 0.125, Test Accuracy= 0.062\n",
            "Step 500, Minibatch Loss= 0.3547, Training Accuracy= 0.906, Test Accuracy= 0.906\n",
            "Model saved in path: /tmp/model.ckpt\n",
            "Step 1000, Minibatch Loss= 0.2223, Training Accuracy= 0.922, Test Accuracy= 0.945\n",
            "Model saved in path: /tmp/model.ckpt\n",
            "Step 1500, Minibatch Loss= 0.4307, Training Accuracy= 0.906, Test Accuracy= 0.961\n",
            "Model saved in path: /tmp/model.ckpt\n",
            "Step 2000, Minibatch Loss= 0.1025, Training Accuracy= 0.938, Test Accuracy= 0.977\n",
            "Model saved in path: /tmp/model.ckpt\n",
            "Step 2500, Minibatch Loss= 0.2563, Training Accuracy= 0.891, Test Accuracy= 0.961\n",
            "Step 3000, Minibatch Loss= 0.1184, Training Accuracy= 0.969, Test Accuracy= 0.969\n",
            "Step 3500, Minibatch Loss= 0.1279, Training Accuracy= 0.953, Test Accuracy= 0.969\n",
            "Step 4000, Minibatch Loss= 0.0419, Training Accuracy= 0.984, Test Accuracy= 0.984\n",
            "Model saved in path: /tmp/model.ckpt\n",
            "Step 4500, Minibatch Loss= 0.3604, Training Accuracy= 0.938, Test Accuracy= 0.969\n",
            "Step 5000, Minibatch Loss= 0.0682, Training Accuracy= 0.984, Test Accuracy= 0.977\n",
            "Optimization Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chQz9B84y-he"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}